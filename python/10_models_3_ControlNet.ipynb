{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diffusion \n",
    "\n",
    "## Inference 4: ControlNet\n",
    "\n",
    "\n",
    "## Workflow\n",
    "\n",
    "#### Drive\n",
    "\n",
    "If you need to load/save to your drive:\n",
    "\n",
    "```python\n",
    "import sys\n",
    "if 'google.colab' in sys.modules:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive/')\n",
    "\n",
    "import os\n",
    "os.chdir('drive/My Drive/IS53055B-DMLCP/DMLCP/python') # to change to another directory\n",
    "```\n",
    "\n",
    "#### Huggingface login\n",
    "\n",
    "For some models and datasets, and if you want to push your model to HF (same as GitHub, but for models) you need to be logged into your HF account.\n",
    "\n",
    "For that, you need to create an account [here](https://huggingface.co/) and then to ['/settings/tokens'](https://huggingface.co/settings/tokens) to create an access token.\n",
    "\n",
    "```python\n",
    "from pathlib import Path\n",
    "from huggingface_hub import notebook_login\n",
    "if not (Path.home()/'.huggingface'/'token').exists():\n",
    "    notebook_login()\n",
    "```\n",
    "\n",
    "#### Install\n",
    "\n",
    "1. On Colab, just use `pip` to install Huggingface libraries (see below).\n",
    "\n",
    "2. Locally, the install is the same as the one used for Language models, see [`setup.md`](https://github.com/jchwenger/DMLCP/blob/main/setup.md#pytorch--huggingfacegradio)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "if 'google.colab' in sys.modules:\n",
    "    !pip install --upgrade transformers diffusers accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running stable diffusion\n",
    "To run stable diffusion you will need to distinguish wether you are running this on a Mac M1/M2 or on Linux/Windows with a Nvidia GPU. \n",
    "On a M1/M2 Mac, diffusion will need a \"warmup\" phase to work properly (see [this link](https://huggingface.co/docs/diffusers/optimization/mps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Get cpu, gpu or mps device for training.\n",
    "# See: https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html#creating-models\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import cv2\n",
    "from skimage import feature\n",
    "\n",
    "from diffusers import ControlNetModel\n",
    "from diffusers import StableDiffusionPipeline\n",
    "from diffusers import StableDiffusionControlNetPipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now this will load the pretrained model and download it the first time the cell is run (which might take a while)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipe = StableDiffusionPipeline.from_pretrained(\n",
    "    \"runwayml/stable-diffusion-v1-5\",\n",
    ").to(device)  # to accelerator\n",
    "\n",
    "pipe.safety_checker = None # remove NSFW filter (too many innocuous prompts yield an empty image when I test it)\n",
    " # Note: this is no licence to do harm, it is to give *you* the responsibility\n",
    " # of your use. (Also, the HF safety_checker is very, very conservative, and rejects\n",
    " # a lot of abstract images.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"A cubist painting of the Star Treck character Spock, high quality, trending on artstation\"\n",
    "\n",
    "SAVE_MEMORY = False\n",
    "\n",
    "if SAVE_MEMORY:                     # Saves memory at the cost of speed:\n",
    "    pipe.enable_attention_slicing() #  https://huggingface.co/docs/diffusers/main/en/api/diffusion_pipeline#diffusers.DiffusionPipeline.enable_attention_slicing \n",
    "\n",
    "if device==\"mps\":                           # First-time \"warmup\" pass for M1/M2 macs\n",
    "    _ = pipe(prompt, num_inference_steps=1) # https://huggingface.co/docs/diffusers/v0.4.1/en/optimization/mps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the image (note increasing `num_inference_steps` will improve quality but be slower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "image = pipe(prompt, num_inference_steps=20).images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.array(image))\n",
    "plt.title(prompt)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditioning stable diffusion with ControlNet\n",
    "\n",
    "[ControlNet](https://stablediffusionweb.com/ControlNet) ([repo](https://github.com/lllyasviel/ControlNet), [documentation](https://huggingface.co/docs/diffusers/api/pipelines/controlnet)) is a very recent and quite amazing advancement in image generation using stable diffusion. It allows conditioning the stable diffusion generation pipeline on an image input (similarly to pix2pix).\n",
    "\n",
    "You also can see it running [as a space on Huggingface](https://huggingface.co/spaces/hysts/ControlNet-v1-1) for testing and forking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "controlnet = ControlNetModel.from_pretrained(\n",
    "    \"lllyasviel/sd-controlnet-canny\",\n",
    "    torch_dtype=torch.float16\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use skimage to create edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_canny_skimage(img, sigma=1.5, invert=False):\n",
    "    grey_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    edges = (feature.canny(grey_img, sigma=sigma)*255).astype(np.uint8)\n",
    "    if invert:\n",
    "        edges = cv2.bitwise_not(edges)\n",
    "    return cv2.cvtColor(edges, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "# accessing the source image on Colab without connecting to the Drive\n",
    "if 'google.colab' in sys.modules:\n",
    "    import requests\n",
    "    from io import BytesIO\n",
    "    url = \"https://raw.githubusercontent.com/jchwenger/DMLCP/main/python/images/spock.jpg\"\n",
    "    response = requests.get(url)\n",
    "    img = np.array(Image.open(BytesIO(response.content)))\n",
    "# if run locally, assuming you do so from the repo\n",
    "else:\n",
    "    img = plt.imread(\"images/spock.jpg\") # can be any image\n",
    "    \n",
    "edges =  apply_canny_skimage(img)\n",
    "\n",
    "# ControlNet expects a PIL Image as input\n",
    "edges_image = Image.fromarray(edges)\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.subplot(121)\n",
    "plt.imshow(img)\n",
    "plt.subplot(122)\n",
    "plt.imshow(edges)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup the ControlNet model. Somehow converting to device takes ages on Mac."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = StableDiffusionControlNetPipeline.from_pretrained(\n",
    "    \"runwayml/stable-diffusion-v1-5\",\n",
    "    torch_dtype=torch.float16,\n",
    "    controlnet=controlnet, \n",
    "    safety_checker=None\n",
    ").to(device) # to accelerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"A sculpture made of clay on a white background\"\n",
    "\n",
    "if SAVE_MEMORY:                     # Saves memory at the cost of speed:\n",
    "    pipe.enable_attention_slicing() # https://huggingface.co/docs/diffusers/main/en/api/diffusion_pipeline#diffusers.DiffusionPipeline.enable_attention_slicing \n",
    "\n",
    "if device=='mps':                           # First-time \"warmup\" pass for M1/M2 macs\n",
    "    _ = pipe(prompt, num_inference_steps=1) # https://huggingface.co/docs/diffusers/v0.4.1/en/optimization/mps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = pipe(\n",
    "    prompt,\n",
    "    image=edges_image,\n",
    "    num_inference_steps=30,\n",
    ").images[0]\n",
    "\n",
    "plt.imshow(np.array(image))\n",
    "plt.title(prompt)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
