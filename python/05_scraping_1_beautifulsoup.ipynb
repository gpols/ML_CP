{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9i6LqS2cvMjF"
   },
   "source": [
    "# Scraping I\n",
    "\n",
    "---\n",
    "\n",
    "## BeautifulSoup\n",
    "\n",
    "See the official [quickstart](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#quick-start). And [this tutorial](https://realpython.com/beautiful-soup-web-scraper-python/).\n",
    "\n",
    "If you need to load/save to your drive:\n",
    "\n",
    "```python\n",
    "import sys\n",
    "if 'google.colab' in sys.modules:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive/')\n",
    "\n",
    "import os\n",
    "os.chdir('drive/My Drive/IS53055B-DMLCP/DMLCP/python') # change to your directory\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DyF273kVvJtn"
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n45WMRklvL7a"
   },
   "outputs": [],
   "source": [
    "html_doc = \"\"\"<html><head><title>The Dormouse's story</title></head>\n",
    "<body>\n",
    "<p class=\"title\"><b>The Dormouse's story</b></p>\n",
    "\n",
    "<p class=\"story\">Once upon a time there were three little sisters; and their names were\n",
    "<a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\">Elsie</a>,\n",
    "<a href=\"http://example.com/lacie\" class=\"sister\" id=\"link2\">Lacie</a> and\n",
    "<a href=\"http://example.com/tillie\" class=\"sister\" id=\"link3\">Tillie</a>;\n",
    "and they lived at the bottom of a well.</p>\n",
    "\n",
    "<p class=\"story\">...</p>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pfeoUgSDvu_v"
   },
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zB51ZcCywNaG"
   },
   "source": [
    "Render straight into the notebook!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KqjLFPIwv-iE"
   },
   "outputs": [],
   "source": [
    "import IPython # https://stackoverflow.com/a/55329863\n",
    "IPython.display.HTML(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zwv5VnpZwdQa"
   },
   "source": [
    "Now we can programmatically navigate the webpage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "02wyZW3gwWr9"
   },
   "outputs": [],
   "source": [
    "soup.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3dDtZ5h1wY4j"
   },
   "outputs": [],
   "source": [
    "soup.title.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pOxWtDjhwZ9y"
   },
   "outputs": [],
   "source": [
    "soup.title.string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U6kkgJ3iwa7d"
   },
   "outputs": [],
   "source": [
    "soup.title.parent.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mpMLvzMVwbuq"
   },
   "outputs": [],
   "source": [
    "soup.p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-Mv_ifjRwq02"
   },
   "outputs": [],
   "source": [
    "soup.p['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tE35oGAXwrVR"
   },
   "outputs": [],
   "source": [
    "soup.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9P5z6ZzqwskK"
   },
   "outputs": [],
   "source": [
    "soup.find_all('a') # also try 'p'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kBPhvnHbwyWI"
   },
   "outputs": [],
   "source": [
    "soup.find(id=\"link3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uuu6Vqi7w095"
   },
   "source": [
    "Extract links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6xAQAKdXw1-W"
   },
   "outputs": [],
   "source": [
    "for link in soup.find_all('a'):\n",
    "    print(link.get('href'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zeO83E2hxE4G"
   },
   "source": [
    "Extract only the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZD1N-O1uxHiW"
   },
   "outputs": [],
   "source": [
    "print(soup.get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i8W8tMpnxOg8"
   },
   "source": [
    "If you wanted to save this text in Python, you would do:\n",
    "\n",
    "```python\n",
    "with open(\"dormhouse-story.txt\") as o: # open file object\n",
    "    o.write(soup.get_text())           # write the text\n",
    "```    \n",
    "\n",
    "Don't forget to read (or, more like, search for stuff in) the [documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#quick-start)! (Or ask ChatGPT...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B2Uwu6Q4y6kp"
   },
   "source": [
    "---\n",
    "\n",
    "## Important Note: Download Webpages in Python\n",
    "\n",
    "Above we had the html code at hand. But of course IRL we want to grab it from the web!\n",
    "\n",
    "For that we can use [requests](https://pypi.org/project/requests/).\n",
    "\n",
    "(See the [mentioned tutorial](https://realpython.com/beautiful-soup-web-scraper-python/#step-2-scrape-html-content-from-a-page).)\n",
    "\n",
    "In Colab it's already installed, otherwise (**in an environment!**):\n",
    "\n",
    "```bash\n",
    " conda install -c anaconda requests\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8zNWt0ErzlUn"
   },
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H6KW6EhyznOu"
   },
   "outputs": [],
   "source": [
    "URL = \"https://en.wikipedia.org/wiki/Artificial_intelligence\"\n",
    "page = requests.get(URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WBEcEUWO0GPC"
   },
   "outputs": [],
   "source": [
    "page # response 200 means OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZGEnrFjR0KGi"
   },
   "outputs": [],
   "source": [
    "print(page.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rh-TrXIf0jee"
   },
   "source": [
    "Like before, we can display the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XEcLEOXT0PNi"
   },
   "outputs": [],
   "source": [
    "IPython.display.HTML(page.text) # not sure why the images didn't get fetched"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ihfg3brO06_S"
   },
   "source": [
    "We can also do that with [urllib](https://docs.python.org/3/howto/urllib2.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZuMHsvMa06jC"
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "with urllib.request.urlopen(URL) as response:\n",
    "    html = response.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FXPPwnaf3oXa"
   },
   "outputs": [],
   "source": [
    "# IPython.display.HTML(html.decode()) # will print the page as before (with images!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s6dSpE9eIn15"
   },
   "source": [
    "The main skill to have now in order to scrape successfully is, actually, [**html**](https://www.w3schools.com/html/)! That is, understand how webpages are constructed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6sC0zZtq2lf2"
   },
   "source": [
    "## Grab images\n",
    "\n",
    "Everything has to be done manually, trial and error.\n",
    "\n",
    "What we do:\n",
    "- We check all `img` tags, and get their `src`\n",
    "- We inspect the links, see if they work out of the box or not\n",
    "- We correct the links, and try and request their contents\n",
    "- We find the way to save that to files\n",
    "\n",
    "### Note: this process is error-prone and sometimes *painful*!\n",
    "\n",
    "Don't underestimate the time and learning you will need to do this, it's a huge chunk of the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WUvLxeL_1ovu"
   },
   "outputs": [],
   "source": [
    "wiki_soup = BeautifulSoup(html) # works the same with page.content from earlier\n",
    "images_links = []\n",
    "for img in wiki_soup.find_all('img'):\n",
    "    # print(img)\n",
    "    images_links.append(img.get('src'))\n",
    "\n",
    "for i in images_links:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XP5RKOi38uGg"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "d = 'scraped-images'\n",
    "os.mkdir(d)\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cC4mDN964UXZ"
   },
   "outputs": [],
   "source": [
    "import time   # time module for pausing programme\n",
    "import shutil # OS module for saving a stream of bytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wn4MWEGhEpQO"
   },
   "source": [
    "Helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HLYbTPoY5VjK"
   },
   "outputs": [],
   "source": [
    "def make_request(link):\n",
    "    r = requests.get(link, stream=True)\n",
    "    if r.status_code == 200:\n",
    "        # print('got it!')\n",
    "        return r\n",
    "    else:\n",
    "        # print('nope)\n",
    "        return None\n",
    "\n",
    "def make_filename(link):\n",
    "    idx = link.rfind('/') # find the last /\n",
    "    return link[idx+1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NTcHH7vX3zcm"
   },
   "outputs": [],
   "source": [
    "wiki_url = 'https://en.wikipedia.org'\n",
    "\n",
    "print('attempting to scrape:')\n",
    "for l in images_links:\n",
    "    print(f'- {l}')\n",
    "                                                             # three types of links (found by trial and error!)\n",
    "    if l.startswith('//'):                                   # - ones with //, used as is\n",
    "        l = l.replace('//', '') # remove the leading dash\n",
    "        l = f\"https://{l}\"\n",
    "        resp = make_request(l)\n",
    "    elif l.startswith('http'):                               # - ones requiring nothing\n",
    "        resp = make_request(l)\n",
    "    else:                                                    # - ones requiring to add the leading wiki url\n",
    "        l = f\"{wiki_url}{l}\"\n",
    "        resp = make_request(l)\n",
    "\n",
    "    if resp is not None:                                     # if we got something\n",
    "        fname = make_filename(l)                             # get the filename\n",
    "        print(f'  attempting to save {fname}')\n",
    "        with open(os.path.join(d, fname), 'wb') as o:        # saving logic, see here: https://towardsdatascience.com/a-tutorial-on-scraping-images-from-the-web-using-beautifulsoup-206a7633e948\n",
    "            resp.raw.decode_content = True                   #                         https://stackoverflow.com/a/29328036\n",
    "            shutil.copyfileobj(resp.raw, o)                  #                         https://stackoverflow.com/a/13137873\n",
    "    else:\n",
    "        print(f'  could not retrieve this one')\n",
    "\n",
    "    time.sleep(1) # BE NICE, let the server breathe and space out your calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ni5QiCtgE_Av"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "Image.open(os.path.join(d, 'wikipedia.png')) # display one of our downloaded images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z53zwIv5HjSf"
   },
   "source": [
    "---\n",
    "\n",
    "## Next steps\n",
    "\n",
    "BeautifulSoup will not be able to handle much interactivity in website (for example if you need to click on something to open the page). The next level for scraping is then to use a *headless browser* that you can automate (devilish, really).\n",
    "\n",
    "The tool for that is [Selenium](https://selenium-python.readthedocs.io/installation.html#installing-python-bindings-for-selenium). Note that you not only need to install the library, but also the *driver* that will pass on the commands to whichever browser you wish to use (Chrome, Chromium, Firefox, etc.) The website has an intro and tutorial."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
