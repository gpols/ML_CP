{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Timbre transfer with RAVE\n",
    "\n",
    "NOTE: This notebook is heavily (almost completely) adapted by a notebook created by Teresa Pelinski 2023\n",
    "\n",
    "RAVE is a Real-time Audio Variational autoEncoder (https://github.com/acids-ircam/RAVE) released by Caillon and Esling (ACIDS IRCAM) in November 2021. You can read the paper here: https://arxiv.org/abs/2111.05011. RAVE is a particularly light model that allows generating audio in real-time in the CPU and even in embedded systems with low computational power, such as Raspberry Pi (here is a video: https://youtu.be/jAIRf4nGgYI). Still, training this model is computationally expensive: in the original paper, they used 3M steps, which took six days on a TITAN V GPU. \n",
    "\n",
    "Today we will use RAVE to perform timbre transfer on sounds downloaded from freesound.org (although you can use sounds from other sources if you prefer!). First, we will download the pretrained RAVE models from the internet. Then we will perform timbre transfer on some sample sounds and experiment with biasing the latent space. Finally, we will see some audio transformations we can do to combine and modify sounds. \n",
    "\n",
    "The task for this lab is to generate a 30sâ€“1min composition by combining sounds. These sounds can be a combination of downloaded from freesound, generated with RAVE, modified with the techniques below or a combination of original and timbre transfer. \n",
    "\n",
    "\n",
    "\\* If you are interested in using RAVE for performing, the real-time implementation runs in MaxMSP and can be downloaded here: https://github.com/acids-ircam/nn_tilde\n",
    "\n",
    "\\* The RAVE generation code in this notebook is based on https://colab.research.google.com/github/hdparmar/AI-Music/blob/main/Latent_Soundings_workshop_RAVE.ipynb\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Setup\n",
    "Make sure you are running this notebook in the `dmlap` conda environment."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installs and imports\n",
    "\n",
    "From the terminal. First make sure that your DMLAP conda environment is active, e.g. \n",
    "```\n",
    "conda activate dmlap\n",
    "```\n",
    "Then install the required dependencies with\n",
    "```\n",
    "pip install acids-rave \n",
    "conda install -c conda-forge ffmpeg\n",
    "pip install wget\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import IPython.display as ipd\n",
    "import librosa as li\n",
    "import soundfile as sf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import wget\n",
    "import os\n",
    "import sys\n",
    "from scipy import signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Timbre transfer\n",
    "### Download pretrained models\n",
    "Some info on the pretrained models is available here: https://acids-ircam.github.io/rave_models_download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_path = \"./models/rave-pretrained-models\" # folder where pretrained models will be downloaded\n",
    "if not os.path.exists(pt_path): # create the folder if it doesn't exist\n",
    "    os.mkdir(pt_path)\n",
    "    \n",
    "def bar_progress(current, total, width=80): # progress bar for wget\n",
    "    progress_message = \"Downloading: %d%% [%d / %d] bytes\" % (current / total * 100, current, total)\n",
    "    # Don't use print() as it will print in new line every time.\n",
    "    sys.stdout.write(\"\\r\" + progress_message)\n",
    "    sys.stdout.flush()\n",
    "\n",
    "pretrained_models = [\"vintage\", \"percussion\", \"nasa\", \"darbouka_onnx\", \"VCTK\"] # list of available pretrained_models to download in https://acids-ircam.github.io/rave_models_download (you can select less if you want to spend less time on this cell)\n",
    "\n",
    "for model in pretrained_models: # download pretrained models and save them in pt_path\n",
    "    if not os.path.exists(os.path.join(pt_path, f\"{model}.ts\")): # only download if not already downloaded\n",
    "        print(f\"Downloading {model}.ts...\")\n",
    "        wget.download(f\"https://play.forum.ircam.fr/rave-vst-api/get_model/{model}\",f\"{pt_path}/{model}.ts\", bar=bar_progress)\n",
    "    else:\n",
    "        print(f\"{model}.ts already downloaded\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load an audio file and listen to it\n",
    "We can load an audio file using librosa (`li`). `li.load` returns an array where every item corresponds to the amplitude at each time sample. You can convert from time in samples to time in seconds using `time = np.arange(0, len(input_data))/sample_rate`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "sample_rate = 48000 # sample rate of the audio\n",
    "\n",
    "input_file = \"./sounds/trumpet.wav\"  \n",
    "sound_format = os.path.splitext(input_file)[1]\n",
    "input_data = li.load(input_file, sr=sample_rate)[0] # load input audio\n",
    "\n",
    "time = np.arange(0, len(input_data)) / sample_rate # to obtain the time in seconds, we need to divide the sample index by the sample rate\n",
    "plt.plot(time,input_data)\n",
    "plt.xlabel(\"Time (seconds)\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "plt.title(input_file.split(\"/\")[-1])\n",
    "plt.grid()\n",
    "\n",
    "ipd.display(ipd.Audio(data=input_data, rate=sample_rate)) # display audio widget"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the model and generate\n",
    "We can now load a pretrained model using `torch.jit.load` and encode the input audio into a latent representation.For the vintage model, we will be encoding our input audio into a latent space trained on the [VCTK](https://datashare.ed.ac.uk/handle/10283/2950) dataset. It consists speech from 109 native speakers of English with various accents. We can then decode the latent representation an synthesise it. This will make the original sound as if it was made of speech sounds (timbre transfer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_path = \"generated\" # folder where generated audio will be saved\n",
    "if not os.path.exists(generated_path): # create the folder if it doesn't exist\n",
    "    os.mkdir(generated_path)\n",
    "    \n",
    "pretrained_model =  \"VCTK\" # select the pretrained model to use. VCTK \n",
    "\n",
    "model = torch.jit.load(f\"{pt_path}/{pretrained_model}.ts\" ).eval() # load model\n",
    "torch.set_grad_enabled(False) # disable gradients\n",
    "    \n",
    "x = torch.from_numpy(input_data).reshape(1, 1, -1) # convert audio to tensor and add batch and channel dimensions\n",
    "z = model.encode(x) # encode audio into latent representation\n",
    "# synthesize audio from latent representation\n",
    "y = model.decode(z).numpy() # decode latent representation and convert tensor to numpy array\n",
    "y = y[:,0,:].reshape(-1) # remove batch and channel dimensions\n",
    "\n",
    "\n",
    "# also there seems to be a delay this function should take care of that \n",
    "def align_generated_signal(input_data, y, thresh=0.05):\n",
    "    # The generated signal also has an initial silence, also not sure about the reason\n",
    "    mask = np.abs(y) < thresh\n",
    "    if mask.any() and thresh >= 0:\n",
    "        idx = np.argmax(~mask)\n",
    "        y = np.roll(y, -idx)\n",
    "    offset = abs(len(input_data)- len(y))\n",
    "    y = y[:-offset] # trim to match input length --> for some reason the output is a bit longer than the input\n",
    "    return y\n",
    "y = align_generated_signal(input_data, y)\n",
    "\n",
    "# save output audio\n",
    "output_file =f'{generated_path}/{input_file.replace(\".wav\", f\"_{pretrained_model}_generated.wav\").split(\"/\")[-1]}'\n",
    "print(output_file)\n",
    "\n",
    "sf.write(output_file, y, sample_rate)\n",
    "ipd.Audio(output_file) # display audio widget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compare the input and output sound wave and spectogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import signal\n",
    "f1, t1, Zxx1 = signal.stft(input_data, fs=sample_rate, nperseg=2048, noverlap=512)\n",
    "f2, t2, Zxx2 = signal.stft(y, fs=sample_rate, nperseg=2048, noverlap=512)\n",
    "\n",
    "fig, axs = plt.subplots(2, 2,figsize=(10,5), sharex=True)\n",
    "\n",
    "axs[0,0].plot(time,input_data)\n",
    "axs[0,0].set_ylabel(\"Amplitude\")\n",
    "axs[0,0].grid()\n",
    "axs[0,0].set_title(input_file.split(\"/\")[-1])\n",
    "axs[1,0].plot(time,y)\n",
    "axs[1,0].set_ylabel(\"Amplitude\")\n",
    "axs[1,0].set_xlabel(\"Time (seconds)\")\n",
    "axs[1,0].grid()\n",
    "axs[1,0].set_title(output_file.split(\"/\")[-1])\n",
    "\n",
    "axs[0,1].pcolormesh(t1, f1[:100], np.abs(li.amplitude_to_db(Zxx1[:100,:],\n",
    "                                                       ref=np.max)))\n",
    "axs[1,1].pcolormesh(t2, f2[:100], np.abs(li.amplitude_to_db(Zxx2[:100,:],\n",
    "                                                       ref=np.max)))\n",
    "axs[1,1].set_xlabel(\"Time (seconds)\")\n",
    "axs[0,1].set_title(\"STFT\")\n",
    "axs[0,1].set_ylabel(\"Frequency (Hz)\")\n",
    "axs[1,1].set_ylabel(\"Frequency (Hz)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Sound transformations\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alter latent representation\n",
    "We can now modify the latent coordinates of the input file to alter the representation. We can start by adding a constant bias (a displacement) to the coordinates in the latent space. Note that each RAVE model has a different number of coordinates for its latent space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "print(z.shape) # the second dimension corresponds to the latent dimension, in this case, there's 8 latent dimensions\n",
    "\n",
    "d0 = 2.2  # change in latent dimension 0\n",
    "d1 = 0.4\n",
    "d2 = 1\n",
    "d3 = 0.5 \n",
    "# we leave dimensions 4-8 unchanged\n",
    "\n",
    "z_modified = torch.clone(z) # copy latent representation\n",
    "# bias latent dimensions (displace each sample representation by a constant value)\n",
    "z_modified[:, 0] += torch.linspace(d0,d0, z.shape[-1])\n",
    "z_modified[:, 1] += torch.linspace(d1,d1, z.shape[-1])\n",
    "z_modified[:, 2] += torch.linspace(d2,d2, z.shape[-1])\n",
    "z_modified[:, 3] += torch.linspace(d3,d3, z.shape[-1])\n",
    "\n",
    "y_latent_1 = model.decode(z_modified).numpy() # decode latent representation and convert tensor to numpy array\n",
    "\n",
    "y_latent_1 = y_latent_1[:,0,:].reshape(-1) # remove batch and channel dimensions\n",
    "y_latent_1 = align_generated_signal(input_data, y_latent_1) # align \n",
    "output_file = f'{generated_path}/{input_file.replace(\".wav\", f\"_{pretrained_model}_latent_generated_1.wav\").split(\"/\")[-1]}'\n",
    "sf.write(output_file,y_latent_1, sample_rate) # save output audio\n",
    "\n",
    "ipd.Audio(output_file) # display audio widget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of using a constant (a bias) to displace the representation of every sample in the latent space, we can use a function so that we \"navigate\" the latent space. For example, we can use a sinusoidal function that the representation oscillates around the original encoded one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_modified = torch.clone(z) # copy original latent representation\n",
    "\n",
    "# bias latent dimensions with a sinusoidal function at 440 Hz\n",
    "t = torch.linspace(0, z.shape[-1], z.shape[-1])\n",
    "for idx in range(0, z.shape[1]): # for each latent dimension\n",
    "    z_modified[:, idx] += torch.sin(440*2*np.pi*t)\n",
    "\n",
    "y_latent_2 = model.decode(z_modified).numpy() # decode latent representation and convert tensor to numpy array\n",
    "y_latent_2 = y_latent_2[:,0,:].reshape(-1) # remove batch and channel dimensions\n",
    "#print(abs(len(input_data) - len(y_latent_2)))\n",
    "#y_latent_2 = y_latent_2[abs(len(input_data) - len(y_latent_2))] # trim to match input length\n",
    "y_latent_2 = align_generated_signal(input_data, y_latent_2) # align \n",
    "output_file = f'{generated_path}/{input_file.replace(\".wav\", f\"_{pretrained_model}_latent_generated_1.wav\").split(\"/\")[-1]}'\n",
    "sf.write(output_file,y_latent_2, sample_rate) # save output audio\n",
    "\n",
    "ipd.Audio(output_file) # display audio widget\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mix sounds (sum sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixed_output = y + input_data*0.5\n",
    "sf.write(output_file,mixed_output, sample_rate)\n",
    "ipd.Audio(output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sound collage\n",
    "\n",
    "To generate your final composition, you should combine various sounds extracts. For this, you can cut excerpts of audio files, pass them through RAVE and combine them in a collage audio file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate three sounds -- example using a for loop\n",
    "\n",
    "# Segments of audio to concatenate.\n",
    "# The format is:\n",
    "#             (filename, amplitude, model,  start_sample, end_sample)\n",
    "# Make sure that end_sample is larger than start sample\n",
    "segments = [(\"sounds/trumpet.wav\", 1.0, \"VCTK\", sample_rate, 4*sample_rate),\n",
    "            (\"sounds/happypiano.wav\", 1.0, \"VCTK\", 0, 4*sample_rate),\n",
    "            (\"sounds/happypiano.wav\", 1.0, \"darbouka_onnx\", 0, 4*sample_rate),\n",
    "            (\"sounds/violin-scale.wav\", 1.0, \"VCTK\", 0, 4*sample_rate),\n",
    "            (\"sounds/violin-scale.wav\", 1.0, \"percussion\", 0, 4*sample_rate),\n",
    "            ]\n",
    "\n",
    "outputs = [] # here we will store the output audio\n",
    "inputs = []  # here we will store the input audio\n",
    "\n",
    "for index, segment in enumerate(segments):\n",
    "    input_file, amp, model_name, start_sample, end_sample = segment\n",
    "    input_data = li.load(input_file, sr=sample_rate)[0][int(start_sample):int(end_sample)] # load input audio and cut from start to end sample\n",
    "    input_data *= amp # Set amplitude\n",
    "    inputs = np.append(inputs,input_data) # add excerpt to inputs array\n",
    "    \n",
    "    # load model\n",
    "    model = torch.jit.load(f\"{pt_path}/{model_name}.ts\").eval() # load moel\n",
    "    torch.set_grad_enabled(False)\n",
    "        \n",
    "    # encode input audio to latent representation\n",
    "    x = torch.from_numpy(input_data).reshape(1, 1, -1)\n",
    "    z = model.encode(x)\n",
    "\n",
    "    # synthesize audio from latent representation\n",
    "    y = model.decode(z).numpy()\n",
    "    y = y[:,0,:].reshape(-1) # remove batch and channel dimensions\n",
    "\n",
    "    #y = y[abs(len(input_data)- len(y)):] # trim to match input length\n",
    "    y = align_generated_signal(input_data, y) # Use this as an alternative to align also start, you may want to play with the thresh optional parameter\n",
    "    \n",
    "    outputs = np.append(outputs,y) # append to output array\n",
    "\n",
    "input_file = f'{generated_path}/input_collage.wav'\n",
    "output_file = f'{generated_path}/output_collage.wav'\n",
    "sf.write(output_file,outputs+0.5*inputs, sample_rate) # save output audio (sum input and ouput audio, input with less volume)\n",
    "ipd.Audio(output_file)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "b569cc9be53083e787d8b1313a26e0731e8a98a84352215d2f1ca78ae62b88e2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
